{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Langchain Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path for pdfs and chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf(s) with the help of PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='MUHAMEDHARISH\n",
      "+ 9 1 7 9 0 4 4 8 1 5 7 2 | m u h a m e d m h a r i s h @ g m a i l . c o m | Link edIn | G i t H u b SKILLSLanguages: Python, C++, SQLLibraries/Frameworks: Keras, Tensorflow, Pandas, NumPY, Matplotlib, Scikit-Learn, Flask, OpenCV, DjangoTools/Platforms:Docker, Git, GitHub, HuggingFaceAPI, Ollama, GoogleCloud, Firebase\n",
      "EDUCATIONSRMInstituteofScienceofTechnology||Vadapalani, ChennaiSeptember2021–June2025B.TechComputerScienceRelevantCoursework: DataStructuresandAlgorithms, OperatingSystems, ComputerNetworking, DatabaseManagement Systems, BigDataToolsandTechniques, Artificial Intelligence, SoftwareEngineering, Object-OrientedProgrammingAwards:Winnerof IETPentathon2023-HackathonCategory\n",
      "WORKEXPERIENCE\n",
      "EvolveRobotLab: GenerativeAIIntern09/2024–10/2024● Collaboratedinthedevelopment of aTamil-languagechatbot bydeployingviaDockeronGoogleCloud, offeringvoice-to-text responsesinTamil toenhanceaccessibilityandengagement fornativespeakers.● Appliedfoundational conceptsinAIandmachinelearningtodesignauser-friendlyinterfaceforreal-timechatbotinteraction, tailoredspecificallytolocal languagenuances.● Deployedthechatbot toproduction, achieving100%uptimethroughrobust Dockercontainerizationandcloud-baseddeployment, facilitatingcontinuous, uninterrupteduserengagement.\n",
      "LanguifyAI: MachineLearningEngineerIntern09/2022–12/2022● Developedaconvolutional neural network(CNN)model usingtheFashionMNISTdataset, achievinga95%accuracyinautomatedbiascorrectionthroughdeeplearningtechniques.● Assistedinimplementingstatistical analysismethodsthat helpedreducebiasinresponsegeneration, leadingtoa25%improvement inrecommendationaccuracy.\n",
      "PROJECTS\n",
      "CustomerResponseModel|Python, Transformers, GPT-2, TensorFlow|GitHub● DevelopedaQuestionAnsweringmodel forgeneratingresponsestocustomerqueriesusingadvancednatural languageprocessing(NLP)techniques.● Fine-tunedGPT-2fordomain-specificresponses, improvingaccuracyandrelevanceincustomerinteractions.● Assessedthemodel'sperformanceusingmetricssuchasBLEUscoretoevaluatethequalityof generatedresponses.\n",
      "LocalRetrieval-AugmentedGenerationModel|Mistral, Ollama, LangChain, ChromaVectorDB, Streamlit |GitHub● Built aRAGmodel usingMistral andOllamafordocument retrieval andansweringuserqueriesbyintegratingexternalknowledgefromPDFfiles.● UtilizedLangChainandChromaVectorDBforefficient indexing, retrieval, andoptimizedembedding-basedsearch.● DeployedthesolutionusingOllamalocal hostingandStreamlit forseamlessreal-timeuserexperienceforinterface.\n",
      "VideoRecommendationSystem|Python, Pandas, NumPy, Scikit-Learn, PostmanAPI |GitHub● Designedarecommendationsystemthat suggestspersonalizedvideosbasedoncontent similarityusingfeatureextractiontechniques.● Implementedcontent-basedfilteringalgorithmswithScikit-Learn, optimizingrecommendationsforabetteruserexperience.● Conductedunit testingwithPostmanAPItoverifyreliabilityof recommendationsundercertainrequirements.' metadata={'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader= PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "documents = load_documents()\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the document is created, split the texts into number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='MUHAMEDHARISH\n",
      "+ 9 1 7 9 0 4 4 8 1 5 7 2 | m u h a m e d m h a r i s h @ g m a i l . c o m | Link edIn | G i t H u b SKILLSLanguages: Python, C++, SQLLibraries/Frameworks: Keras, Tensorflow, Pandas, NumPY, Matplotlib, Scikit-Learn, Flask, OpenCV, DjangoTools/Platforms:Docker, Git, GitHub, HuggingFaceAPI, Ollama, GoogleCloud, Firebase\n",
      "EDUCATIONSRMInstituteofScienceofTechnology||Vadapalani, ChennaiSeptember2021–June2025B.TechComputerScienceRelevantCoursework: DataStructuresandAlgorithms, OperatingSystems, ComputerNetworking, DatabaseManagement Systems, BigDataToolsandTechniques, Artificial Intelligence, SoftwareEngineering, Object-OrientedProgrammingAwards:Winnerof IETPentathon2023-HackathonCategory\n",
      "WORKEXPERIENCE' metadata={'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the chunks are created, establish embedding for them to inculcate into database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self, model_name=\"nomic-embed-text\"):\n",
    "        self.embeddings = OllamaEmbeddings(model=model_name)\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        # Assuming OllamaEmbeddings has an embed method, you can call it here\n",
    "        return self.embeddings.embed(query)\n",
    "    \n",
    "    def embed_documents(self, documents):\n",
    "        return self.embeddings.embed_documents(documents)\n",
    "\n",
    "# Now use this class in your Chroma setup\n",
    "embedding_function = TextEmbedder()  # Use the default model \"nomic-embed-text\"\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDB function if there is no need for addition or updation of PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating individual ID's for chunks in the format of source/page number/chunks Index. This solves the problem of having to create new database when just wanting to add content/update pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    #first we gather source and page from all chunks to make a simple page/source ID\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "        #since several chunks share the same page ID, we create a condition for chunk index count\n",
    "        #this defines if the page id is same, increase the chunk index count\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        #this defines if the page is different, reset the chunk index to 0\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        #Unique ID in desired format\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "        \n",
    "\n",
    "        # Add it to the page meta-data as an element\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks\n",
    "#This creates the desired chunks with ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the chunks by calling the id creation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first chunk to see if it gave us the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: data\\AI_Resume2025.pdf:0:0, Metadata: {'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'id': 'data\\\\AI_Resume2025.pdf:0:0'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_to_check = chunks_with_ids[0]  \n",
    "print(f\"Chunk ID: {chunk_to_check.metadata['id']}, Metadata: {chunk_to_check.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document updation function if new PDFs are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # This is only to add or update more chunks \n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\") #length must be 0 if run first without existing DB\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    #Runs if there are new chunks\n",
    "    if len(new_chunks):\n",
    "        print(f\"👉 Adding new documents: {len(new_chunks)}\")\n",
    "        #create new unique IDs for new chunks\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        #Adds new chunk along with its Ids to the database\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #for saving and future use \n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument parse for terminal handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil \n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    documents = load_documents()\n",
    "    chunks = split_documents(documents)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "👉 Adding new documents: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91979\\AppData\\Local\\Temp\\ipykernel_29496\\1219488333.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextEmbedder' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m documents \u001b[38;5;241m=\u001b[39m load_documents()\n\u001b[0;32m      5\u001b[0m chunks \u001b[38;5;241m=\u001b[39m split_documents(documents)\n\u001b[1;32m----> 6\u001b[0m \u001b[43madd_to_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36madd_to_chroma\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m     24\u001b[0m new_chunk_ids \u001b[38;5;241m=\u001b[39m [chunk\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m new_chunks]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Adds new chunk along with its Ids to the database\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_chunk_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#for saving and future use \u001b[39;00m\n\u001b[0;32m     28\u001b[0m db\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32mc:\\Users\\91979\\anaconda3\\envs\\rag\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:288\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    287\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\91979\\anaconda3\\envs\\rag\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextEmbedder' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Response ---\n",
      "\n",
      " The context provided does not include any information about the skills of Muhamed Harish. Therefore, it is impossible to answer the question based solely on this context.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from embed_function import text_embed\n",
    "\n",
    "# Define paths and constants\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# Prompt templates\n",
    "QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant tasked with improving the retrieval of relevant documents. \n",
    "Generate five different versions of the given user question to maximize the relevance of retrieved documents.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag_with_multiquery(query_text: str):\n",
    "    # Load the Chroma vector database\n",
    "    embedding_function = text_embed()\n",
    "    vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = OllamaLLM(model=\"mistral-openorca\")\n",
    "    \n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=QUERY_PROMPT_TEMPLATE,\n",
    "    )\n",
    "    \n",
    "    # Initialize the retriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(),\n",
    "        llm,\n",
    "        prompt=query_prompt,\n",
    "    )\n",
    "\n",
    "    # Generate the context using the retriever\n",
    "    documents = retriever.invoke(query_text)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Create the RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    final_prompt = rag_prompt.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Invoke the LLM with the RAG prompt\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test the query RAG with multiquery function directly in the notebook\n",
    "query_text = \"What are the skills of Muhamed Harish?\"\n",
    "\n",
    "# Call the function to get a response\n",
    "response = query_rag_with_multiquery(query_text)\n",
    "print(\"\\n--- Final Response ---\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
