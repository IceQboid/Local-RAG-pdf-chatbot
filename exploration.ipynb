{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Langchain Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path for pdfs and chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf(s) with the help of PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended December 31, 2021\n",
      "OR\n",
      "☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from _________ to _________\n",
      "Commission File Number: 001-34756\n",
      "Tesla, Inc.\n",
      "(Exact name of registrant as specified in its charter)\n",
      "  \n",
      "Delaware  91-2197729\n",
      "(State or other jurisdiction ofincorporation or organization)  (I.R.S. EmployerIdentification No.)\n",
      "  \n",
      "13101 Tesla RoadAustin, Texas   78725\n",
      "(Address of principal executive offices)  (Zip Code)\n",
      "(512) 516-8177\n",
      "(Registrant’s telephone number, including area code)\n",
      "Securities registered pursuant to Section 12(b) of the Act:\n",
      "  \n",
      "Title of each class Trading Symbol(s) Name of each exchange on which registered\n",
      "Common stock TSLA The Nasdaq Global Select Market\n",
      " \n",
      "Securities registered pursuant to Section 12(g) of the Act:\n",
      "None\n",
      "Indicate by check mark whether the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.    \n",
      "Yes  ☒     No  ☐ \n",
      "Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or 15(d) of the Act.    Yes  ☐     \n",
      "No  ☒ \n",
      "Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the \n",
      "Securities Exchange Act of 1934 (“Exchange Act”) during the preceding 12 months (or for such shorter period that the registrant \n",
      "was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days.    Yes  ☒     No  ☐ \n",
      "Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be \n",
      "submitted pursuant to Rule 405 of Regulation S-T (§232.405 of this chapter) during the preceding 12 months (or for such shorter \n",
      "period that the registrant was required to submit such files).    Yes  ☒     No  ☐ \n",
      "Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a \n",
      "smaller reporting company, or an emerging growth company. See the definitions of “large accelerated filer,” “accelerated filer,” \n",
      "“smaller reporting company” and “emerging growth company” in Rule 12b-2 of the Exchange Act:\n",
      "  \n",
      "Large accelerated filer   ☒       Accelerated filer   ☐ \n",
      "                  \n",
      "Non-accelerated filer   ☐       Smaller reporting company   ☐ \n",
      "                  \n",
      "Emerging growth company   ☐             \n",
      "  \n",
      "If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period \n",
      "for complying with any new or revised financial accounting standards provided pursuant to Section 13(a) of the Exchange Act. ☐ \n",
      " \n",
      "7/8/25, 9:04 AM 10-K\n",
      "https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm 1/120' metadata={'producer': 'Skia/PDF m138', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36', 'creationdate': '2025-07-08T03:34:30+00:00', 'title': '10-K', 'moddate': '2025-07-08T03:34:30+00:00', 'source': 'data\\\\10-K.pdf', 'total_pages': 120, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader= PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "documents = load_documents()\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the document is created, split the texts into number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended December 31, 2021\n",
      "OR\n",
      "☐ TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from _________ to _________\n",
      "Commission File Number: 001-34756\n",
      "Tesla, Inc.\n",
      "(Exact name of registrant as specified in its charter)\n",
      "  \n",
      "Delaware  91-2197729\n",
      "(State or other jurisdiction ofincorporation or organization)  (I.R.S. EmployerIdentification No.)\n",
      "  \n",
      "13101 Tesla RoadAustin, Texas   78725\n",
      "(Address of principal executive offices)  (Zip Code)\n",
      "(512) 516-8177\n",
      "(Registrant’s telephone number, including area code)\n",
      "Securities registered pursuant to Section 12(b) of the Act:\n",
      "  \n",
      "Title of each class Trading Symbol(s) Name of each exchange on which registered\n",
      "Common stock TSLA The Nasdaq Global Select Market' metadata={'producer': 'Skia/PDF m138', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36', 'creationdate': '2025-07-08T03:34:30+00:00', 'title': '10-K', 'moddate': '2025-07-08T03:34:30+00:00', 'source': 'data\\\\10-K.pdf', 'total_pages': 120, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the chunks are created, establish embedding for them to inculcate into database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self, model_name=\"nomic-embed-text\"):\n",
    "        self.embeddings = OllamaEmbeddings(model=model_name)\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        # Assuming OllamaEmbeddings has an embed method, you can call it here\n",
    "        return self.embeddings.embed(query)\n",
    "    \n",
    "    def embed_documents(self, documents):\n",
    "        return self.embeddings.embed_documents(documents)\n",
    "\n",
    "# Now use this class in your Chroma setup\n",
    "embedding_function = TextEmbedder()  # Use the default model \"nomic-embed-text\"\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDB function if there is no need for addition or updation of PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating individual ID's for chunks in the format of source/page number/chunks Index. This solves the problem of having to create new database when just wanting to add content/update pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    #first we gather source and page from all chunks to make a simple page/source ID\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "        #since several chunks share the same page ID, we create a condition for chunk index count\n",
    "        #this defines if the page id is same, increase the chunk index count\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        #this defines if the page is different, reset the chunk index to 0\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        #Unique ID in desired format\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "        \n",
    "\n",
    "        # Add it to the page meta-data as an element\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks\n",
    "#This creates the desired chunks with ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the chunks by calling the id creation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first chunk to see if it gave us the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: data\\10-K.pdf:0:0, Metadata: {'producer': 'Skia/PDF m138', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36', 'creationdate': '2025-07-08T03:34:30+00:00', 'title': '10-K', 'moddate': '2025-07-08T03:34:30+00:00', 'source': 'data\\\\10-K.pdf', 'total_pages': 120, 'page': 0, 'page_label': '1', 'id': 'data\\\\10-K.pdf:0:0'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_to_check = chunks_with_ids[0]  \n",
    "print(f\"Chunk ID: {chunk_to_check.metadata['id']}, Metadata: {chunk_to_check.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document updation function if new PDFs are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # This is only to add or update more chunks \n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\") #length must be 0 if run first without existing DB\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    #Runs if there are new chunks\n",
    "    if len(new_chunks):\n",
    "        print(f\"👉 Adding new documents: {len(new_chunks)}\")\n",
    "        #create new unique IDs for new chunks\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        #Adds new chunk along with its Ids to the database\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #for saving and future use \n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument parse for terminal handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil \n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    documents = load_documents()\n",
    "    chunks = split_documents(documents)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91979\\AppData\\Local\\Temp\\ipykernel_10048\\1219488333.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 4\n",
      "👉 Adding new documents: 576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91979\\AppData\\Local\\Temp\\ipykernel_10048\\1219488333.py:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Response ---\n",
      "\n",
      " The risk factor indicates that there are various risks and uncertainties associated with the company's business, financial condition, and future results. These risks include the impact of COVID-19 pandemic, foreign currency risk, supply risk, inventory valuation risk, maintaining and expanding international operations, and implementation of new systems. The risks described in this report are not exhaustive, as there might be other material risks not currently known to the company or deemed immaterial at the time.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from embed_function import text_embed\n",
    "\n",
    "# Define paths and constants\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# Prompt templates\n",
    "QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant tasked with improving the retrieval of relevant documents. \n",
    "Generate five different versions of the given user question to maximize the relevance of retrieved documents.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag_with_multiquery(query_text: str):\n",
    "    # Load the Chroma vector database\n",
    "    embedding_function = text_embed()\n",
    "    vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = OllamaLLM(model=\"mistral-openorca\")\n",
    "    \n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=QUERY_PROMPT_TEMPLATE,\n",
    "    )\n",
    "    \n",
    "    # Initialize the retriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(),\n",
    "        llm,\n",
    "        prompt=query_prompt,\n",
    "    )\n",
    "\n",
    "    # Generate the context using the retriever\n",
    "    documents = retriever.invoke(query_text)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Create the RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    final_prompt = rag_prompt.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Invoke the LLM with the RAG prompt\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test the query RAG with multiquery function directly in the notebook\n",
    "query_text = \"What does the risk factor indicate?\"\n",
    "\n",
    "# Call the function to get a response\n",
    "response = query_rag_with_multiquery(query_text)\n",
    "print(\"\\n--- Final Response ---\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
