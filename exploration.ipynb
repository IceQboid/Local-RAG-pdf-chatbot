{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Langchain Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path for pdfs and chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf(s) with the help of PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='MUHAMEDHARISH\n",
      "+ 9 1 7 9 0 4 4 8 1 5 7 2 | m u h a m e d m h a r i s h @ g m a i l . c o m | Link edIn | G i t H u b SKILLSLanguages: Python, C++, SQLLibraries/Frameworks: Keras, Tensorflow, Pandas, NumPY, Matplotlib, Scikit-Learn, Flask, OpenCV, DjangoTools/Platforms:Docker, Git, GitHub, HuggingFaceAPI, Ollama, GoogleCloud, Firebase\n",
      "EDUCATIONSRMInstituteofScienceofTechnology||Vadapalani, ChennaiSeptember2021–June2025B.TechComputerScienceRelevantCoursework: DataStructuresandAlgorithms, OperatingSystems, ComputerNetworking, DatabaseManagement Systems, BigDataToolsandTechniques, Artificial Intelligence, SoftwareEngineering, Object-OrientedProgrammingAwards:Winnerof IETPentathon2023-HackathonCategory\n",
      "WORKEXPERIENCE\n",
      "EvolveRobotLab: GenerativeAIIntern09/2024–10/2024● Collaboratedinthedevelopment of aTamil-languagechatbot bydeployingviaDockeronGoogleCloud, offeringvoice-to-text responsesinTamil toenhanceaccessibilityandengagement fornativespeakers.● Appliedfoundational conceptsinAIandmachinelearningtodesignauser-friendlyinterfaceforreal-timechatbotinteraction, tailoredspecificallytolocal languagenuances.● Deployedthechatbot toproduction, achieving100%uptimethroughrobust Dockercontainerizationandcloud-baseddeployment, facilitatingcontinuous, uninterrupteduserengagement.\n",
      "LanguifyAI: MachineLearningEngineerIntern09/2022–12/2022● Developedaconvolutional neural network(CNN)model usingtheFashionMNISTdataset, achievinga95%accuracyinautomatedbiascorrectionthroughdeeplearningtechniques.● Assistedinimplementingstatistical analysismethodsthat helpedreducebiasinresponsegeneration, leadingtoa25%improvement inrecommendationaccuracy.\n",
      "PROJECTS\n",
      "CustomerResponseModel|Python, Transformers, GPT-2, TensorFlow|GitHub● DevelopedaQuestionAnsweringmodel forgeneratingresponsestocustomerqueriesusingadvancednatural languageprocessing(NLP)techniques.● Fine-tunedGPT-2fordomain-specificresponses, improvingaccuracyandrelevanceincustomerinteractions.● Assessedthemodel'sperformanceusingmetricssuchasBLEUscoretoevaluatethequalityof generatedresponses.\n",
      "LocalRetrieval-AugmentedGenerationModel|Mistral, Ollama, LangChain, ChromaVectorDB, Streamlit |GitHub● Built aRAGmodel usingMistral andOllamafordocument retrieval andansweringuserqueriesbyintegratingexternalknowledgefromPDFfiles.● UtilizedLangChainandChromaVectorDBforefficient indexing, retrieval, andoptimizedembedding-basedsearch.● DeployedthesolutionusingOllamalocal hostingandStreamlit forseamlessreal-timeuserexperienceforinterface.\n",
      "VideoRecommendationSystem|Python, Pandas, NumPy, Scikit-Learn, PostmanAPI |GitHub● Designedarecommendationsystemthat suggestspersonalizedvideosbasedoncontent similarityusingfeatureextractiontechniques.● Implementedcontent-basedfilteringalgorithmswithScikit-Learn, optimizingrecommendationsforabetteruserexperience.● Conductedunit testingwithPostmanAPItoverifyreliabilityof recommendationsundercertainrequirements.' metadata={'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    document_loader= PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "documents = load_documents()\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the document is created, split the texts into number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='MUHAMEDHARISH\n",
      "+ 9 1 7 9 0 4 4 8 1 5 7 2 | m u h a m e d m h a r i s h @ g m a i l . c o m | Link edIn | G i t H u b SKILLSLanguages: Python, C++, SQLLibraries/Frameworks: Keras, Tensorflow, Pandas, NumPY, Matplotlib, Scikit-Learn, Flask, OpenCV, DjangoTools/Platforms:Docker, Git, GitHub, HuggingFaceAPI, Ollama, GoogleCloud, Firebase\n",
      "EDUCATIONSRMInstituteofScienceofTechnology||Vadapalani, ChennaiSeptember2021–June2025B.TechComputerScienceRelevantCoursework: DataStructuresandAlgorithms, OperatingSystems, ComputerNetworking, DatabaseManagement Systems, BigDataToolsandTechniques, Artificial Intelligence, SoftwareEngineering, Object-OrientedProgrammingAwards:Winnerof IETPentathon2023-HackathonCategory\n",
      "WORKEXPERIENCE' metadata={'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the chunks are created, establish embedding for them to inculcate into database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self, model_name=\"nomic-embed-text\"):\n",
    "        self.embeddings = OllamaEmbeddings(model=model_name)\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        # Assuming OllamaEmbeddings has an embed method, you can call it here\n",
    "        return self.embeddings.embed(query)\n",
    "    \n",
    "    def embed_documents(self, documents):\n",
    "        return self.embeddings.embed_documents(documents)\n",
    "\n",
    "# Now use this class in your Chroma setup\n",
    "embedding_function = TextEmbedder()  # Use the default model \"nomic-embed-text\"\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDB function if there is no need for addition or updation of PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function()\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating individual ID's for chunks in the format of source/page number/chunks Index. This solves the problem of having to create new database when just wanting to add content/update pdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    #first we gather source and page from all chunks to make a simple page/source ID\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "        #since several chunks share the same page ID, we create a condition for chunk index count\n",
    "        #this defines if the page id is same, increase the chunk index count\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        #this defines if the page is different, reset the chunk index to 0\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        #Unique ID in desired format\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "        \n",
    "\n",
    "        # Add it to the page meta-data as an element\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks\n",
    "#This creates the desired chunks with ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the chunks by calling the id creation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first chunk to see if it gave us the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: data\\AI_Resume2025.pdf:0:0, Metadata: {'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'AI_Resume2025', 'source': 'data\\\\AI_Resume2025.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'id': 'data\\\\AI_Resume2025.pdf:0:0'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_to_check = chunks_with_ids[0]  \n",
    "print(f\"Chunk ID: {chunk_to_check.metadata['id']}, Metadata: {chunk_to_check.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document updation function if new PDFs are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # This is only to add or update more chunks \n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\") #length must be 0 if run first without existing DB\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    #Runs if there are new chunks\n",
    "    if len(new_chunks):\n",
    "        print(f\"👉 Adding new documents: {len(new_chunks)}\")\n",
    "        #create new unique IDs for new chunks\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        #Adds new chunk along with its Ids to the database\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        #for saving and future use \n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument parse for terminal handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil \n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    documents = load_documents()\n",
    "    chunks = split_documents(documents)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "👉 Adding new documents: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91979\\AppData\\Local\\Temp\\ipykernel_29496\\1219488333.py:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Response ---\n",
      "\n",
      " The best fit role for this candidate would be as a Machine Learning Engineer or a Data Scientist, due to their strong skills in languages like Python and C++, experience with libraries and frameworks related to data processing and AI development, and proven project experience in deploying chatbots and recommendation systems.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from embed_function import text_embed\n",
    "\n",
    "# Define paths and constants\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# Prompt templates\n",
    "QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant tasked with improving the retrieval of relevant documents. \n",
    "Generate five different versions of the given user question to maximize the relevance of retrieved documents.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag_with_multiquery(query_text: str):\n",
    "    # Load the Chroma vector database\n",
    "    embedding_function = text_embed()\n",
    "    vector_db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = OllamaLLM(model=\"mistral-openorca\")\n",
    "    \n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=QUERY_PROMPT_TEMPLATE,\n",
    "    )\n",
    "    \n",
    "    # Initialize the retriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(),\n",
    "        llm,\n",
    "        prompt=query_prompt,\n",
    "    )\n",
    "\n",
    "    # Generate the context using the retriever\n",
    "    documents = retriever.invoke(query_text)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    # Create the RAG prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "    final_prompt = rag_prompt.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Invoke the LLM with the RAG prompt\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test the query RAG with multiquery function directly in the notebook\n",
    "query_text = \"What role is the best fit for this candidate?\"\n",
    "\n",
    "# Call the function to get a response\n",
    "response = query_rag_with_multiquery(query_text)\n",
    "print(\"\\n--- Final Response ---\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
